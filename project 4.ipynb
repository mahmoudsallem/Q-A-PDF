{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import liberary\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "from IPython.display import Markdown\n",
    "from dotenv import load_dotenv\n",
    "import streamlit as st\n",
    "import pinecone\n",
    "import textwrap\n",
    "import os\n",
    "import openai\n",
    "import time\n",
    "\n",
    "\n",
    "def to_markdown(text):\n",
    "  text = text.replace('•', '  *')\n",
    "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets read the documents \n",
    "\n",
    "def read_doc(directory):\n",
    "    fille_loader = PyPDFDirectoryLoader(directory)\n",
    "    documents = fille_loader.load()\n",
    "    return documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = read_doc(\"docs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Divide the docs into chunks\n",
    "\n",
    "def chunk_data(docs,chunk_size=800,chunk_overlap=50):\n",
    "    text_split = RecursiveCharacterTextSplitter(chunk_size=chunk_size,chunk_overlap=chunk_overlap)\n",
    "    doc = text_split.split_documents(docs)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents =  chunk_data(docs=doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\openai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OpenAIEmbeddings(client=<class 'openai.api_resources.embedding.Embedding'>, async_client=None, model='text-embedding-ada-002', deployment='text-embedding-ada-002', openai_api_version='', openai_api_base=None, openai_api_type='', openai_proxy='', embedding_ctx_length=8191, openai_api_key='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', openai_organization=None, allowed_special=set(), disallowed_special='all', chunk_size=1000, max_retries=2, request_timeout=None, headers=None, tiktoken_enabled=True, tiktoken_model_name=None, show_progress_bar=False, model_kwargs={}, skip_empty=False, default_headers=None, default_query=None, retry_min_seconds=4, retry_max_seconds=20, http_client=None)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Embedding using Openai\n",
    "embidding = OpenAIEmbeddings(api_key=os.environ[\"OpenAI_KEY_TOKEN\"])\n",
    "embidding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectours = embidding.embed_query(\"what is RL ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create vector search DB in picone \n",
    "pinecone.init(\n",
    "    api_key=os.environ[\"PINECON_API_KEY\"],\n",
    "    environment=os.environ[\"PINECON_ENVIRONMENT\"]\n",
    ")\n",
    "index_name = \"vectordb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = Pinecone.from_documents(doc,embidding,index_name=index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cosine Similarity retreive Result  from vectour DB\n",
    "def retreve_query(Query,k=2):\n",
    "    matching_result = index.similarity_search(Query,k=k)\n",
    "    return matching_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\openai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.llms.openai.OpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI( model=\"gpt-3.5-turbo-instruct\",temperature=0.5,api_key=os.environ[\"OpenAI_KEY_TOKEN\"])\n",
    "chain = load_qa_chain(llm=llm,chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "> \n",
       "> \n",
       "> RL stands for \"reinforcement learning,\" which is a type of machine learning that involves training an artificial intelligence agent to make decisions based on rewards or punishments received from its environment. This approach is often used in areas such as robotics, game playing, and autonomous vehicles. RL algorithms aim to maximize the total reward obtained by the agent over time, by learning from its interactions with the environment. "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_markdown(llm(\"what is the RL?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## search anserse from vectoure DB\n",
    "\n",
    "def retreve_answer(query):\n",
    "    doc_search = retreve_query(query)\n",
    "    print(doc_search)\n",
    "    response = chain.run(input_documents=doc_search,question=query)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content='Chapter 2 ■ rL theory and aLgorithms\\n21In terms of conditional probabilities, the following is true:\\nT(s,a,s ′)=P(s ′|s,a)T(s,a,s ′)=P(s ′|s,a)\\nR:S×S →RR:S×S →R is a reward function that gives a real number that represents \\nthe amount of reward (or punishment) the environment will grant for a state transition. \\nR(s,s′)R(s,s ′) is the reward received after transitioning to state s ′s′ from state ss.\\nIf the transition model is known to the agent, i.e., the agent knows where it would \\nprobably go from where it stands, it’s fairly easy for the agent to know how to act in a way \\nthat maximizes its expected utility from its experience with the environment.\\nWe can define the expected utility for the agent to be the accumulated rewards it \\ngets throughout its experience with the environment. If the agent goes through the states \\ns0,s1,…,sn−1,sns0,s1,…,sn−1,sn, you could formally define its expected utility as follows:\\n∑nt=1 γtE[R(st −1,st)]∑t=1n γtE[R(st −1,st)]\\nwhere γγ is a discount factor used to decrease the values (and hence the importance) of \\npast rewards, and EE is the expected value .\\nThe problem arises when the agents have no clue about the probabilistic model \\nbehind the transitions, and this where RL comes in. The RL problem can formally be \\ndefined now as the problem of learning a set of parameters in order to maximize the \\nexpected utility.\\nRL comes in two flavors:\\n•\\t Model-based:  The agent attempts to sample and learn the \\nprobabilistic model and use it to determine the best actions it can \\ntake. In this flavor, the set of parameters that was vaguely referred \\nto is the MDP model.\\n•\\t Model-free:  The agent doesn’t bother with the MDP model and \\ninstead attempts to develop a control function that looks at \\nthe state and decides the best action to take. In that case, the \\nparameters to be learned are the ones that define the control \\nfunction.\\nWhere Reinforcement Learning Is Used\\nThis section discusses the different fields of Reinforcement Learning, as shown in \\nFigure\\xa0 2-2.', metadata={'page': 31.0, 'source': 'docs\\\\Reinforcement Learning.pdf'}), Document(page_content='Chapter 2 ■ rL theory and aLgorithms\\n21In terms of conditional probabilities, the following is true:\\nT(s,a,s ′)=P(s ′|s,a)T(s,a,s ′)=P(s ′|s,a)\\nR:S×S →RR:S×S →R is a reward function that gives a real number that represents \\nthe amount of reward (or punishment) the environment will grant for a state transition. \\nR(s,s′)R(s,s ′) is the reward received after transitioning to state s ′s′ from state ss.\\nIf the transition model is known to the agent, i.e., the agent knows where it would \\nprobably go from where it stands, it’s fairly easy for the agent to know how to act in a way \\nthat maximizes its expected utility from its experience with the environment.\\nWe can define the expected utility for the agent to be the accumulated rewards it \\ngets throughout its experience with the environment. If the agent goes through the states \\ns0,s1,…,sn−1,sns0,s1,…,sn−1,sn, you could formally define its expected utility as follows:\\n∑nt=1 γtE[R(st −1,st)]∑t=1n γtE[R(st −1,st)]\\nwhere γγ is a discount factor used to decrease the values (and hence the importance) of \\npast rewards, and EE is the expected value .\\nThe problem arises when the agents have no clue about the probabilistic model \\nbehind the transitions, and this where RL comes in. The RL problem can formally be \\ndefined now as the problem of learning a set of parameters in order to maximize the \\nexpected utility.\\nRL comes in two flavors:\\n•\\t Model-based:  The agent attempts to sample and learn the \\nprobabilistic model and use it to determine the best actions it can \\ntake. In this flavor, the set of parameters that was vaguely referred \\nto is the MDP model.\\n•\\t Model-free:  The agent doesn’t bother with the MDP model and \\ninstead attempts to develop a control function that looks at \\nthe state and decides the best action to take. In that case, the \\nparameters to be learned are the ones that define the control \\nfunction.\\nWhere Reinforcement Learning Is Used\\nThis section discusses the different fields of Reinforcement Learning, as shown in \\nFigure\\xa0 2-2.', metadata={'page': 31.0, 'source': 'docs\\\\Reinforcement Learning.pdf'})]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda3\\envs\\openai\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       ">  Reinforcement Learning is a type of machine learning that involves an agent learning how to act in an environment in order to maximize its expected utility, which is the accumulated rewards it receives throughout its experience. It can be done in two ways: model-based, where the agent learns the probabilistic model of the environment, or model-free, where the agent directly learns a control function."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"what is the RL?\"\n",
    "anwser = retreve_answer(text)\n",
    "to_markdown(anwser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
